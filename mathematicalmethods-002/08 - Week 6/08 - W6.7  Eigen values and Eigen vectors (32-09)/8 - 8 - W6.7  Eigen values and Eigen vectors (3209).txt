The next section is eigenvalues and
eigenvectors.
And I'm going to go back to looking at the
covariance matrix I talked
about in let's see it was last, last
Wednesdays' lecture.
So let's start out with a rectangular
matrix R, so it has m rows and n columns.
And then I'm going to define R tilde.
This is exactly the same R tilde that I
had last Wednesday.
So remember we could make this matrix
operator that had the
effect of subtracting the sample mean from
each column of R.
So R tilde is just a matrix whose columns
each sum up to zero.
And then I'm going to take the, so this
is the singular value factorization of
this matrix R tilde.
[BLANK_AUDIO]
And now remember that I could make my
covariance matrix.
So the covariance matrix for my original
matrix R, is 1
divided by m minus 1 times R tilde
transpose R tilde.
And now I have this singular value
factorization for R tilde.
So generally for homework assignments and
any time
you see these, when you, when you have
a factorization for a matrix, the very
next
step is to replace your matrix with its
factorization.
So, I'm going to replace R tilde now, with
U sigma V transposes, so I end up
with my co-variance matrix, also being
equal to one divided
by M minus one.
And now I have these two singular value,
or it's one singular value factorization
of the matrix R tilde.
And I'm going to use my rules for a
transpose of a product of matrices.
So I want to take the transpose of this
quantity U sigma V transpose.
And what I'm going to end up with, so the
V
transpose gets transposed and ends up in
the first spot.
So I end up with a v.
Sigma gets a transpose, but it's in the
middle
so it's just going to stay in the same
spot.
And then U is going to get transposed and
end up in the final spot.
And then I'm going to put some, because
I'm free
to do the matrix multiplications in
whatever order I like.
I'm going to do this U transpose U first.
And since U is an orthogonal matrix, U
transpose U is equal to the identity
matrix.
And so this is just going to go away.
And so what I'll end up with is V sigma
transpose, sigma
V transpose.
And you have to be a little bit careful
here because if, I'd like to just say this
is something like sigma squared.
But the dimensions aren't correct so
remember sigma has
to has the same dimensions as the original
matrix
R, so R is M by N, so it's not a square
matrix.
Then even though you kind of think of a
diagonal matrix as having
symmetry built in, you can't just say that
it's equal to its transpose.
Because the dimensions won't work, even
though
the part that doesn't work is all zero.
So I have to leave this middle bit as
sigma transpose sigma.
Oops.
But
I can look at that and make a little bit
more progress.
So if I look at sigma it's diagonal with
singular values down the main diagonal.
And now I'm just leaving everything that's
zero blank.
So what I mean by this is I'm allowed
non-zero
values and they have to be decreasing down
the main diagonal.
And they have to be decreasing just
because
that's sort of the convention for singular
values.
And everything
that has i not equal to j is forced to be
zero.
And then notice the, the brackets go down
a little bit further
than the last one and that's this kind of
empty bit down here.
It's just to signify that there's these
extra.
M minus n rows that are all zeros.
And then when I take the
transpose of this, so the sigma transpose
is now just going to have that
extra block of all zeros on the right
here.
But when I start making this product,
what's going to happen is I have sigma 1.
Times sigma one and then I'm going to
carry on across this row and
carry on down this column and that's just
going to be zeros multiplied together.
So the one, one element of this, and this
is going to
end up now sigma transpose is going to be
an n by
m matrix.
So the product will be m by n.
And the top left element is going to be
sigma 1 squared.
And then as I continue doing this
multiplication, what's going to
happen is any time I do an off diagonal
multiplication here.
Whatever singular values on this side is
going to hit a zero over here.
And whatever singular value is over here
is going to hit a zero over here.
So all
of the off-diagonal elements of this are
going to be
zero, and it's only when I hit the
diagonal elements.
So I do the, say the second row here times
the second column here.
I'll have zero times zero plus sigma two
times sigma two, so sigma two squared.
And then plus a whole bunch of zeros
afterwards.
So I'm going to end up with a square
matrix,
an n by n matrix, that has the squares of
these singular values down the diagonal.
So sigma transpose sigma is a square
matrix, with
a square of the singular values down the
diagonal.
Okay, so that's what I just said.
And then, because they were already
positive,
we still have the same ordering property.
And so now I'm going to define a matrix
called lambda.
To be 1 over m minus 1.
So remember, from the previous slide, I
was carrying
this 1 over m minus 1 factor with me.
[COUGH]
Times sigma transpose sigma.
And so what I'm going to end up with, are
the diagonal elements of this matrix,
are just going to be the squares of the
singular values, divided by m minus 1.
And it's only going to have a non zero
entry wherever this matrix had a non zero
entry and we just said that that's only
going to have non zero entries down the
diagonal.
So I'm going to rename this matrix sigma
transpose sigma.
This is a capital lambda, and then the
diagonal
elements of my capital lambda, which it's
a diagonal matrix.
The diagonal elements are going to be
called lambda one,
lambda two, lambda three, and so on down
to lambda n.
And now if I substitute this matrix,
lambda, into my expression for the
covariance matrix.
So,
I had, remember I started out with my
covariance matrix for my matrix r.
And now I'm able to get rid of this
1/(m-1)
in the sigma transpose sigma because
that's how I defined lambda.
Since this is a scalar value, one over x
minus one, I can move it across these
matrices to do the multiplication.
And so now I have a representation
of my covarience matrix for r.
As V lambda V,
oops, V transposed.
So, I can write it as an orthogonal
matrix times a diagonal matrix times the
transpose of the same orthogonal matrix.
So, this is just something I'm going to
need for, for
the rest of this slide, and for the next
few calculations.
So I've used previously the letter e to
mean a vector of all ones.
When I give it a subscript, so I say e sub
j, I
mean a vector that's 1 in the Jth element
and 0 everywhere else.
So e1 would be 1, 0, 0, 0 up to the length
of the vector.
And if you think about what that is, just
in, in geometry, if I was talking
about a, a 2-by-2 space.
So the of vectors of length 2, e1 would be
the vector 1, 0, which would just point in
the direction of the x-axis.
e2 would be 0, 1, which would just be
the vector pointing in the direction of
the y-axis.
So that's what I mean when I say e sub j
is a unit vector in the Jth
coordinate direction.
So coordinate one would be the x
direction.
Coordinate two is the y direction and so
on.
[COUGH]
And now I want to think about what's
going to happen, so I take
my covariance matrix.
And I multiply it by one of
the singular, one of these right singular
vectors that are in this matrix, v.
So, these, it's it's getting a bit
complicated.
These are right singular vectors of R
tilde, but this
is, just this one special vector that that
I like there.
At this point, it's probably better just
to think
of them as the columns of this matrix V.
So Vj is the Jth column of the matrix V.
And what's going to happen when
I multiply my covariance matrix times Vj.
Well
I have this way of writing my covariance
down though.
So I'm going to replace the covariance
matrix by V lambda V transpose.
Times Vj and now lets think about what's
going to happen here.
So V transpose has rows, v1, v2 v3 and so
on.
Each one of those is perpendicular to V
sub j except
for the Jth row, which is exactly the
same.
So when I do this, I'm going to get zeroes
everywhere except the Jth element,
which is going to be Vj transposed times
Vj, which is going to be equal to 1.
And that's exactly what I, I said this ej
would be.
So this is that, that, sort of, first
picture I had in the
last set of slides, where I'm sort of
rotating
my system to line up with the coordinate
directions.
So, I have V transposed times V sub j, and
that's just going to be e sub j.
And now lets think about what happens
when I multiply a times a vector V.
So this is just going to be one of the
ways
I can think about matrix multiplication
was as a linear combination.
So the answer for this matrix
multiplication problem is
V1 times the first column of A, plus V2
times
the second column of A, and so on.
And now the, the matrix operation and
multiplication operation I want to look
at, is lambda times e sub j.
And so the ej's, that's whats giving me
the coefficients here and all of these
are equal to 0 except for the Jth one.
So all this matrix multiplication
is going to do, it's going to give me the
Jth column of
this matrix lambda.
So lambda times e sub j is just going to
be, so its zero times every other column,
so it's 1 times the Jth column.
And then the Jth column, it's 0 everywhere
up here,
0 everywhere down here, it's only in the
Jth position
that it's non-zero and the value is lambda
sub j.
And so I can also write that as just
lambda sub j, which is
a scaler, times a vector that's equal to 1
only in the jth position.
So this lambda, so this matrix times this
vector e sub j
is equal to lambda times that same vector,
e sub j, the
little lambda for corresponding to that
direction, so
this lambda sub j, e sub j, so now I can
substitute that.
So just to sort of recap we are, I started
out with my covariance matrix.
Times one of these vectors, one of the
columns of V, the Jth column of V.
And I've worked that now to, worked that
down to V times lambda times e sub j.
But then I can write this matrix lambda
times e sub j.
That's also equal to just this number
times the vector e sub j.
So one number is certainly a lot easier to
deal with than a whole matrix.
So making some progress.
And then because that's a, a scalar.
So lambda j is just the number, that's a
scalar value,
I can bring that out, I can bring that out
in front.
So I can move that across the matrix.
And so now the last matrix vector product
I have to consider is V times e sub j.
And remember this is just a linear
combination of the columns of V,
and e sub j is equal to 0 everywhere
except for the Jth position.
So basically what's going to happen when I
do this matrix multiplication, I get zero
times the first column of v, plus zero
times the second column of v.
And then I keep doing that until I finally
get one times the Jth column of
V, plus 0 times the J+1th column of V, and
so on, up to n.
And so the J column of V is just this
vector V sub j again.
So, what ended up happening, it's all sort
of summed
up in one line, I have a matrix, some
special directions.
And when I multiple this matrix, the
co-variance matrix for R, times
one of these special directions V sub j, I
get a vector that points in
exactly the same direction and it's just
scaled by a value lambda
sub J.
So for this special
direction V sub j, multiplication by a
matrix is
equal, it's the same operation, as just a
scaling
operation, as multiplication by a scaler
value.
So we started out with this v sub j being
a right singular vector
of R tilde.
Define the covariance matrix to be R tilde
transpose R tilde.
[COUGH]
And then by doing that if we have the
right singular vector of this
matrix R tilde, when I multiply that by
the covariance matrix it's
just going to scale that vector, it's
going to stretch it.
So I end up with the covariance of R times
this vector
v sub j points in the same direction of s
sub j and
it scaled by a factor lambda.
So this was a very special case that I
just wanted
to sort of go through to show where these
things are coming from.
So, in general, if, so this is only, what
I've done here is only for a covariance
matrix, so only for this matrix that I can
write as the product of R tilde transposed
and R tilde.
in general, if I have a square matrix, so,
my covariance matrix was a square matrix.
If A is a square matrix and I want to
think about the product A times a vector
x, there are going to be certain special
directions,
x, that are the same direction as A sub x.
And these directions are called the
eigenvectors of A.
So there, there, are the directions
there, when I multiply by A, I only scale
the vector, I don't change the directions
[UNKNOWN].
it's not always guaranteed that these
things exist.
[COUGH]
But, for covariance matrices, they do.
Yeah?
>>
[INAUDIBLE].
>> So there's two kinds of singular
vectors.
There's the left and right.
and if I make my square matrix by taking R
tilde transpose R,
then it turns out that the right singular
vectors are, are the
eigenvectors of this matrix here, the
covariance matrix.
But the singular vectors
have to be orthoganol.
Right?
Because that was the definition of my
singular value decomposition.
If I make my square matrix as a product
of,
you know, transpose of a rectangular
matrix times a rectangular matrix.
Then I end up having orthogonal
eigenvectors, but that's not the
case in general, so if A is a matrix that
I can't write like this.
Then there's no guarantee that it's going
to have eigenvectors, and or
that they would be orthogonal to one
another Is that a good enough answer?
Okay.
So for these special directions so the
eigenvector directions
the matrix product so A times an
eigenvector is equal to lambda so
the lambda is specific for this direction
x times that vector.
So there the special directions for matrix
times that direction is
just scaling that direction and the amount
of scaling is lambda sub-x
so the scaling for the direction x
The scaling for the direction x is called
the eigenvalue.
So you have an eigenvector as the
direction and the
eigenvalue is the amount of scaling that
occurs in that direction.
And so we can use this to do something
called diagonalizing a matrix.
And I'm able to diagonalize that matrix,
so it has to be a square matrix,
so I'm going to think of A being n by n
now, and if it has n linearly independent
eigenvectors.
So if there are, is one eigenvector for.
Sort of each dimension of my matrix,
and all of those factors are linearly
independent.
Then I can put those
eigenvectors in a matrix S, so S is a
matrix whose columns are the n
eigenvectors of A.
And then if I pre-multipy A by S inverse
And post multiple A by S I end up
with this matrix lambda that has the eigen
values down the diagonal.
And so the idea here is if I know what the
eigenvectors are I can
take a square matrix and I can turn it
into a diagonal matrix.
So in this case,
this is sort of the general idea s just
has to have these eigenvectors in it.
For my covariance matrix we had the extra
added property that s inverse
because the eigenvectors were
orthothogonal s
inverse would be equal to s transpose.
So, if I have these N linearly
independent eigenvectors, I can
diagonalize A like this.
And then the sort of, useful ways of
writing this down when you're trying to do
calculations with eigenvectors,
eigenvalues are, you can have
A times S is equal to S lambda it's.
Essentially just the combinations you can
get by looking at this
and then pre or post multiplying by S or S
inverse.
I tend to think of this as the most
natural one because it's writing it down
as a factorization.
So I'm
factoring A into a matrix of eigenvectors.
This diagonal matrix
of eigenvalues, and then, oops, and then
the inverse of my matrix of eigenvectors.
So diagonalization, there's two separate
things you're
probably going to get confused later on.
So in order to diagonalize, this matrix S
has to have an inverse.
And S is only going to have an inverse, if
my original matrix A has n
linearly independent eigenvectors.
So the, the requirement
for a diagonalization is that A has to
have n eigenvectors.
Later, probably what you're going to want
to do at some point is
use this in place of an inverse because
the diagonal matrix would
be easy to invert and then we'll know that
s has an inverse.
But in order to be able to invert, so in
order to be able to
invert a, so s we know has an inverse
because I have an s here and
an s inverse here.
So we're okay there if I can write the, if
I can diagonalize.
But lambda, I seemed to have changed the
size of my slide, by accident here.
There it goes.
Lambda is a diagonal matrix, and I said at
the very beginning of this diagonal matrix
is only going to have an inverse, if all
of that diagonal elements are non zero.
So invertibility requires that the
diagonal elements, none of
the diagonal elements of Lambda are equal
to 0.
So there are going to be some matrices
that I
can diagonalize but that I won't be able
to invert.
Because they'll have more one or more zero
eigenvalues.
And then the last thing I want to get to,
is
something called the Spectral Theorem, so
if we go back to
my motivating example, which is motivating
example was the Singular value
decomposition of R tilde, and then the
covariance matrix for R.
[COUGH]
So suppose I let A be R tilde
transpose R tilde.
R tilde is an m by n matrix.
Then A is going to be symmetric and so I
think we've already
done this exercise but if you just take
the, the transpose of this.
So if I take A transpose I'll end up with
this R transposed times
this R transposed, transposed so I get the
same thing back.
So I can show that A is equal to A
transpose.
That's the definition of a being
symmetric.
And the Spectral Theorem says that every
symmetric matrix A, so
A being symmetric, just means that A is
equal to A transpose.
So for instance this covariance matrix I
made, by taking the
product R tilde transpose R tilde, that's
a symmetric matrix.
Has a factorization Q lambda Q transpose.
So instead of just having this matrix
S that just has the item vectors in it, I
now have a matrix Q
that's playing the same role as S, but
it's orthogonal.
So S inverse just becomes Q transpose
because for an
orthogonal matrix Q, the inverse is equal
to the transpose.
So this is a much nicer way of writing
this down.
I don't have to compute S inverse.
If I know
what Q is then I immediately know what
Q inverses because there is just the
transpose.
And then I can write the factorization
into the
eigenvectors so Q the eigenvectors and the
eigenvalues
lambda like this with Q inverse equal to Q
transpose.
And so in particular what this is saying
is if A is symmetric I get
this diagonal matrix lambda that has real
eigenvalues in it.
And I get this orthogonal matrix Q that
also has all real entries in it.
And the caveat, so this is just something
to beware of, is
if you have a non-symmetric matrix you can
easily end up with.
So lamba is my shorthand for eigenvalues
and
x is the eigenvectors that are complex
numbers.
So essentially, there's something kind of
going on here
that's saying, when you have a, a
symmetric matrix like
A transpose it sort of has a, a square
root.
Because where do, where do we get complex
numbers from?
It's when we try to take the square root
of a negative number.
So somehow matrices that are symmetric,
that's something
that I can write as a product like this of
real value matrices.
If I have a nonsymmetric matrix, that's
kind of somehow like a negative number,
because if I try to find a matrix like
this that's going to give me.
You know a matrix times itself to get A, I
need complex numbers to do that.
And luckily a lot of the matrices we look
at, so
especially in mathematical finance you're
going
to be looking at covariance matrices.
They are symmetric just because we're
going to compute them like this.
And so they're going to have real
eigenvalues and real eigenvectors.
And then suppose the last, last little
thing
for this section is concept of positive
definite.
And so we'll say a symmetric Matrix is
positive definite if X
transposed Ax is greater than zero or
every other nonzero vector x.
And so this condition it might seem kind
of almost like it's not possible.
But if you have a positive definite matrix
A, so for instance a covariance
instance is going to be a positive
definite matrix, it doesn't matter what
vector.
I can put
a vector of all negative numbers.
I can put a mixed vector of negative
numbers and positive numbers.
but as long as I put x transpose here and
x here,
when I make this product, it's always
going to be greater than 0.
Except for one special case, when x is the
0 vector, and then it'll be exactly equal
to 0.
And so what you end up with is if I have a
symmetric matrix a so I'm just writing
that out as a,b,b,c so the off diagonal
elements have
to be equal to each other that makes a
symmetric.
Then when i look at what this matrix
product is so this x
transpose a x I end up with A x, A x
squared plus, yikes.
What happened here?
So this shoud be A x 1 squared plus 2, 2b
times x1 x2 plus c2 x2 squared.
And that should always be greater than
zero and the easiest way to see that at
least a matrix like this is going to exist
Is that if I put b equals zero, I have
ax squared and cx2 squared, so the little
x is missing here.
And so if I chose a to be greater than 0
and c
to be greater than 0, then this product
would always be greater than 0.
Another way to see that at least one
positive definite
matrix exists is you could just put the
identity matrix here.
That would just give you x transpose x,
which
would be the squared length of the vector,
and of
course that's always going to be positive,
except for
the 0 vector when it would be equal to 0.
So we're going to call this form x
transpose Ax a quadratic form
because it represents this quadratic
function of x1 and x2.
And this is going to have a minimum of
zero at the point
0 and it's going to be positive everywhere
else.
So this is if a is a positive definite
matrix.
And so the, the way to sort of think about
what positive definite is, is Suppose I,
I looked at what would a positive definite
matrix, a positive definite one by one
matrix be?
That would just have to be a positive
number and the way I would think
about that is x times a number times x
would just be say ax squared.
And of course that's going to be positive
when a is
positive and it's going to be negative
when a is negative.
And so a positive definite matrix is just
trying
to do the same thing, but with a matrix.
So a 2 x 2 matrix, a, is positive
definite, is sort of like the
same statement of just saying a, is a
little a here, is a positive number.
And then I have our example for what
I've done with the eigenvector the
eigenvalue composition here.
And essentially we just just end up
getting the same picture when i did the
singuar values and hopefully that's what
we should have expected.
Because we know that this, when I take the
So here I'm taking
the eigen vector eigen value decomposition
of the variance
covariance matrix for R.
And then I'm just saying S is going
to be the eigen vectors and lambda will be
the eigen values.
And so this square root of lambda one,
this is just
the same thing as the first singular value
of the matrix R~
divided by m minus one, square root of m
minus one times the first
singular vector and then the second one,
the orange direction.
So the problem here is that the directions
are kind of up to
plus or minus one.
So I end up with a negative one times the
direction here.
But it's still a vector that's just
pointing in a, in the perpendicular
direction.
To the first eigen vector, and they're
going
to be perpendicular because the
variance-covariance matrix is symmetric.
So I can use the spectral theorem to say
that
the eigen values are going to be an
orthoganal matrix.

