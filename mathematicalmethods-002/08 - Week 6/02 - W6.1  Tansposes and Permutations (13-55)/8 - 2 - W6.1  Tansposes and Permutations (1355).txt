I'll get started, then, with transposes
and permutations.
So let's let A by an M by N matrix.
So, this is a matrix
with M rows, and N columns.
And then I'm going to.
Introduce an operation called the
transpose.
So the transpose of A is just going to be
denoted by A with a little T in the
subscript.
So, you know, just like A squared this is
A transpose.
And that just means so the transpose
operation just puts the columns
So the columns of A transposed are the
rows of A, so the
first row of A becomes the first column of
A transposed, the second
row of A becomes the second column of A
transposed and so on.
And if you think about it in terms of the
the
elements so a matrix A has elements that
are little a,
sub i j.
In the transpose those would just be a sub
j i so I'm just swapping the indeces.
And so if I do that if the dimension of my
matrix a is m by n then if
I swap the row and the columns, the
dimension of a transpose is
going to be n by m.
So here I had n columns.
They become the n rows of a transpose.
And just, I mean it's a, it's a pretty
obvious thing to do, if I have
a as just one, two, three, four, five, six
so the first row is one, two, three.
And the first column of A transpose is
one, two, three.
And when A is not square, not that the
dimension changes too.
So it goes from being a two by three
matrix.
So A has two rows and three columns.
A transpose then has three rows and two
columns.
And so some properties of the transpose,
the first one, hope everybody agrees with
this.
So if I take A transpose and transpose it
again, I just get A back.
If I have a
transpose
of a sum,
so I have
A plus B
transpose,
thats
just going
to be equal to
[INAUDIBLE]
it's not
letting me
highlight
there,
has to
be the
same as
whatever
is in the
3 1 position
and in
the,
the 4
is here
have to
be same,
so that
would
be 3 2
position
has to
be the
same as
the 2 3
position So
here I have a and a transposed being the
same thing, so this is a symmetric matrix.
And then an even more special case of
symmetric matrices is a diagonal matrix.
And so a matrix is diagonal if.
Only the diagonal elements are non zero.
So, any element aij where j is not equal
to i has to be equal to zero,
and so a diagonal element just because
these are going to be.
So, this is D11, D22,
D33 if I switch the order of those
subscripts.
They stay the same, so diagonal matrix is
automatically symmetric.
And again, if, the important thing here,
not that
it's symmetric, that should be pretty
obvious, but that
when I have a diagonal matrix, if I take
the transpose of it, I just get the matrix
back.
OK.
And so now I want to explain a
little bit where diagonal matrices are
going to come from.
Sorry, where symmetric matrices are
going to come from.
So let's let R be any m by n matrix.
So here I'm using R just because I want
it to be a rectangular matrix, so, not
necessarily square.
So it has m rows and n columns.
Then the matrix A that I define as the
product of
R transposed and R is going to be a
symmetric matrix.
And so to show that all we have to do is
take the transpose of A and show that
that's equal to A.
So the way I'm going to do that I'll take
A is equal to R transpose R.
So then A transpose is just going to
be the transpose of the product R
transpose R.
And now I want to use the rules that I
gave a
couple of slides ago for how I can deal
with transposes of products.
So basically says I take the transpose of
the
pieces and I get them back in the opposite
order.
So this R here becomes this R transposed
here and this R transposed here, well I
take the transpose of that So it's R
transposed, transposed so it's going to
become R again.
So I have R transposed R and that's what I
initially was calling A,
so I can start out with any rectangular
matrix and if I make this matrix
R transposed R, then that product A is
going to be symmetric matrix.
And you can do something similar for A
equals RR transpose.
Let's see if I, okay, so I go through
this example again, so it's basically the
same idea.
I've defined A to be RR transpose.
And I'm going to take the transpose of
that and if I can show
that to equal to A, then A transpose
equals to A, so A is symentric.
And so the same thing happens, it's just
going to swap the two terms.
So I have R transpose, transposed, so
that's where this first term comes from.
The first piece of this product.
And then here I have an R and then I am
going to transpose it, so I end up with R
transpose, and then the transpose of a
transpose is just I get the matrix back.
So, that will be R, R transpose which
is A, so I get RR transpose is also
symmetric.
And so what we're going to find as we
start looking at matrix
factorizations is a lot of times were
going to start out with a
matrix r that's rectangular, And we're
going to do some calculation where we're
going to end up the product r transpose r,
or r, r transpose.
Or maybe both of them.
And so symmetric matrices are going to
show up
sort of more often than you would expect
just randomly.
And there are a lot of nice properties of
symmetric
matrices that I'll get to later in this
set of lectures.
And the second type of matrix I want to
talk about is called a permutation matrix.
Again, a permutation matrix is going to be
square.
So it's a n by n permutation matrix, P,
has the rows
of, so this is I, the identity matrix but
in any order.
So the identity matrix, it has the, you
know, the first
row, the row that starts with a one, goes
on top.
The row that has a one in the second
position goes in the second place.
And if I shuffle those around in any way,
the matrix that
I end up with is going to be a permutation
matrix.
And so for a 3 by 3 matrices, there's 6
possible permutation matrices, so.
You know, one of the things I could get
when I shuffle
the rows around is, I could just get the
identity matrix back.
So we're actually going to count that as a
permutation matrix that says just put
everything where it is.
And then these other ones, so the one's
I've labeled 21, 3,1 and 3,2.
What these
matrices are going to do when I multiply,
they're just going to swap rows.
So where this would be important, you
know,
when I was talking about pivets when we
doing
elimination it's possible that you could
end up
with a, you could end up with an equation.
There's no particular reason why the
equations have to be in any order.
And so if you had an equation that started
out, so you
had 0 times x plus something times y plus
something times
z equals something, and suppose I decided
to put that equation first.
Then I wouldn't have a pivot in the first
position.
So I probably wouldn't want that to be my
first equation in my system.
And so this is sort of the matrix way of
solving that problem.
If I needed to get that row out of the
first row, I could multiply by one of
these matrices.
So for
instance, I could swap it with the second
row.
And the other interesting that some of
these have is, if I multiply,
P21 by P21, so this is the matrix that
swaps the first two letters.
If I do that again, I would get my
original matrix back.
So that means that P21 is it's own
inverse.
So it swaps row one and two but if I do
that again it puts,
you know, what was originally row one back
in row
one, and what was originally row two back
in row two.
The other interesting property that this
has is P inverse is the same as P
transpose.
So for P21, since this is a symmetric
matrix, and it's its own inverse, you have
this strange...
I guess I would call it duality, but
there's actually three things, so maybe
it's triality.
You have the matrix, its transpose, and
its inverse
all being the same matrix.
for the more complicated ones, so if you
want to swap
you know, more like pairs of rows then you
still have
this property that pre, p inverse is p
transpose, But you
don't have the property that that these
are their own inverses anymore.
As it is the example of what this is going
to do, so
the permutation matrix P32 that just swaps
the third row and the second row.
So if I have a vector 1 2 3, so it's the
column vector, so the
first row is just 1 2 3 Then it's just
going to give me 1, 3, 2.
So it took what ever was in the third
position and will put it in the second
position.
Whatever was in the second position put it
in the third position.
And then if I multiply the output of this,
so 1, 3, 2 by
the same permutation matrix again and it's
just going to swap the 3 and 2.
puts the vector back in the original order
1,2,3.

