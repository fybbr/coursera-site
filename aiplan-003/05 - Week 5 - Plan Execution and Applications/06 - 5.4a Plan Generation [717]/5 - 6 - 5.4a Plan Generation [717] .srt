1
00:00:00,012 --> 00:00:07,183
After the knowledge engineering process,
we should have a planning domain and maybe

2
00:00:07,183 --> 00:00:12,009
planning problems that we can give to a
planner as input.

3
00:00:12,009 --> 00:00:18,984
The planner then does the plan generation,
and that's what we've looked at so far in

4
00:00:18,984 --> 00:00:23,250
this course, algorithms that synthesize
plans.

5
00:00:23,250 --> 00:00:28,280
But the algorithms we have looked at make
some assumptions which may not hold in the

6
00:00:28,280 --> 00:00:33,216
planning problems we are trying to solve.
I will now try to give you an overview of

7
00:00:33,216 --> 00:00:37,961
some other issues and problems that may
come up during plan generation and how

8
00:00:37,961 --> 00:00:42,324
those can be addressed.
Before we talk about different types of

9
00:00:42,324 --> 00:00:48,540
planning problems, here's another approach
to the planning problem as we've described

10
00:00:48,540 --> 00:00:51,757
it so far.
Up to now, we have looked at planning

11
00:00:51,757 --> 00:00:57,469
algorithms that take a planning problem,
apply some algorithm, and the result is

12
00:00:57,469 --> 00:01:01,223
some plan that is a solution to the
planning problem.

13
00:01:01,223 --> 00:01:05,820
Another way to solve planning problems,
the same planning problems, is to

14
00:01:05,820 --> 00:01:10,911
transform the problem into a different
type of problem as a first solution step.

15
00:01:10,911 --> 00:01:15,133
And in this approach here, the problem we
transform it to is a SAT problem, a

16
00:01:15,133 --> 00:01:19,620
satisfiability problem.
I'm not going to describe the details how

17
00:01:19,620 --> 00:01:25,032
this is done here, but SAT problems have
been for around a long time, and in fact,

18
00:01:25,032 --> 00:01:29,261
three SAT was the first problem to be
proved and be complete.

19
00:01:29,261 --> 00:01:33,791
So, there are a number of efficient
solvers that we can use to solve SAT

20
00:01:33,791 --> 00:01:39,095
problems and what they give us is the SAT
solution, which is an assignment of truth

21
00:01:39,095 --> 00:01:42,840
values to propositional variables in the
SAT problem.

22
00:01:42,840 --> 00:01:47,360
And given such a SAT solution, it is
fairly easy to extract a plan from this

23
00:01:47,360 --> 00:01:50,375
solution.
So you can see, this approach is quite

24
00:01:50,375 --> 00:01:55,407
different from everything we've seen
previously, because it doesn't tackle the

25
00:01:55,407 --> 00:02:00,513
problem head on, but transforms it into a
different problem, solves that problem,

26
00:02:00,513 --> 00:02:03,847
and extracts the solution from the other
solution.

27
00:02:03,847 --> 00:02:09,101
And the main reason why this approach
works so well is, of course, that we have

28
00:02:09,101 --> 00:02:13,622
efficient SAT solvers.
This approach was quite successful in the

29
00:02:13,622 --> 00:02:18,817
mid to late '90s, and the planner that
implemented it was called blackbox.

30
00:02:18,817 --> 00:02:25,380
All the planning algorithms we've looked
at so far deal with planning problems

31
00:02:25,380 --> 00:02:31,668
where the actions are deterministic.
That means, if we apply an applicable

32
00:02:31,668 --> 00:02:37,905
action in a given state, we end up in
exactly one other state and that state is

33
00:02:37,905 --> 00:02:41,930
known.
However, there are many problems where the

34
00:02:41,930 --> 00:02:47,650
outcome of actions is uncertain, and if
the outcome of an action is uncertain,

35
00:02:47,650 --> 00:02:53,833
that means, after we apply this action, we
don't know exactly which state we are in.

36
00:02:53,833 --> 00:02:59,667
So, the state we are in is also uncertain.
Now, there are at least two different ways

37
00:02:59,667 --> 00:03:03,090
in which this type of uncertainty can be
dealt with.

38
00:03:03,090 --> 00:03:08,101
The first approach is called belief state
search, and our search space in this

39
00:03:08,101 --> 00:03:13,106
approach is a state of belief states,
where each belief state is really a set of

40
00:03:13,106 --> 00:03:16,741
world states.
And we know that only one of these states

41
00:03:16,741 --> 00:03:21,868
is the actual states, but we don't know
which one, but we know it as one of them.

42
00:03:21,868 --> 00:03:26,202
So that's what we call a belief state,
it's a set of world states.

43
00:03:26,203 --> 00:03:32,220
Then, when we apply an action in a belief
state, of course, the outcome is the union

44
00:03:32,220 --> 00:03:35,724
of all the successors of the individual
states.

45
00:03:35,724 --> 00:03:39,157
So again, it's a belief state, a set of
world states.

46
00:03:39,157 --> 00:03:44,493
So this gives us a search space, as we've
seen it before, and the result then is a

47
00:03:44,493 --> 00:03:48,846
solution plan, which is a sequence of
actions as we have before.

48
00:03:48,846 --> 00:03:53,492
Another approach to dealing with
uncertainty is called contingency

49
00:03:53,492 --> 00:03:57,006
planning.
In contingency planning, what we have is

50
00:03:57,006 --> 00:04:02,126
actions that have multiple possible
outcomes and these action outcomes are

51
00:04:02,126 --> 00:04:06,424
known as contingencies.
Usually, the number of contingencies is

52
00:04:06,424 --> 00:04:10,081
fairly small and not every action must
have contingencies.

53
00:04:10,081 --> 00:04:14,759
So that means, when we search through our
search space, we can still deal with

54
00:04:14,759 --> 00:04:18,430
individual world sets.
But now, our solution plan has a tree

55
00:04:18,430 --> 00:04:23,777
structure that branches where we have take
into account the different contingencies.

56
00:04:23,777 --> 00:04:28,535
And also, our solution plan will contain
observation actions at the branching

57
00:04:28,535 --> 00:04:33,215
points Where we, during execution, observe
which of the branches is actually

58
00:04:33,215 --> 00:04:37,027
happening.
Of course, both these approaches are a lot

59
00:04:37,027 --> 00:04:42,087
more complex than what we've seen so far.
In belief state search, we have sets of

60
00:04:42,087 --> 00:04:46,495
world states as our notes so an
exponential increase in the size of the

61
00:04:46,495 --> 00:04:49,926
search space.
And in contingency planning, we look at

62
00:04:49,926 --> 00:04:54,876
plans that are trees, so they are also a
lot larger than the plans we've looked at

63
00:04:54,876 --> 00:04:59,098
so far.
Sometimes, the degree of uncertainty that

64
00:04:59,098 --> 00:05:03,011
we have in a planning problem can be
quantified.

65
00:05:03,011 --> 00:05:09,545
That is, we know with which probabilities
the different outcomes of an action can

66
00:05:09,545 --> 00:05:12,629
occur.
This approach is called probabilistic

67
00:05:12,629 --> 00:05:15,866
planning and is based on a different
conceptual model.

68
00:05:15,866 --> 00:05:20,772
So we're no longer looking at simple state
transition systems, as we have so far, but

69
00:05:20,772 --> 00:05:25,182
the new conceptual model is called a
Partially Observable Markov Decision

70
00:05:25,182 --> 00:05:27,137
Process.
This one here.

71
00:05:27,137 --> 00:05:30,318
And in this model, we have a lot of
familiar things.

72
00:05:30,318 --> 00:05:35,288
So firstly, we have a set of world states
through which we are searching, and then,

73
00:05:35,288 --> 00:05:38,981
we have a set of actions that give us our
state transitions.

74
00:05:38,981 --> 00:05:43,659
And of course, in some sets, certain
actions are applicable and they must be a

75
00:05:43,659 --> 00:05:46,523
subset of all the actions that are
available.

76
00:05:46,523 --> 00:05:51,463
What we also have is a cost function that
gives the cost of an action applied in a

77
00:05:51,463 --> 00:05:54,617
given state and this cost must be greater
than 0.

78
00:05:54,617 --> 00:06:00,078
So far, this should all look familiar.
But then, in addition to all this, we have

79
00:06:00,078 --> 00:06:05,072
the transition probabilities.
That is we have a function that gives us

80
00:06:05,072 --> 00:06:10,490
the probability for each action that we
end up in state as prime given that we

81
00:06:10,490 --> 00:06:14,616
started in state s.
So this is the probability that we get

82
00:06:14,616 --> 00:06:17,864
from s to s prime when we execute the
action a.

83
00:06:17,864 --> 00:06:23,348
Then, our planning problem also must
include an initial belief state, that is,

84
00:06:23,348 --> 00:06:27,106
a probability distribution over all the
states in s.

85
00:06:27,106 --> 00:06:32,358
So we don't know exactly which state we
start in, but we have a probability

86
00:06:32,358 --> 00:06:37,583
distribution that tells us how likely it
is that we are in a given state.

87
00:06:37,583 --> 00:06:41,494
And, we have a final belief state, which
corresponds to our goal.

88
00:06:41,494 --> 00:06:46,328
Now, the solutions to our probabilistic
planning problems look quite different

89
00:06:46,328 --> 00:06:50,837
from what we've seen so far.
They are no longer action sequences, but

90
00:06:50,837 --> 00:06:56,622
they are something known as a policy.
A policy is effectively a function that

91
00:06:56,622 --> 00:07:02,035
maps states into actions.
Namely, if we are in a given state, that's

92
00:07:02,035 --> 00:07:07,776
the action we should execute.
That most likely leads us to a goal state.

93
00:07:07,776 --> 00:07:14,214
And often, the policy we're looking for is
an optimal policy, which is simply one

94
00:07:14,214 --> 00:07:16,920
that has a minimal expected cost.
